{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ae7595",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "...\n",
    "\n",
    "<img src=\"doc/transformer_cross_attention.png\" alt=\"Transformer Cross Attention\" width=\"500\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be535bc5",
   "metadata": {},
   "source": [
    "### Example 1: learning modulo funcion / groking\n",
    "...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b72c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters\n",
    "# ----------------------------\n",
    "# Vocabulary\n",
    "VOCAB_SIZE = 10 + 1\n",
    "SEP_TOKEN = VOCAB_SIZE - 1\n",
    "MAX_LEN = 3                 # [a, b, SEP]\n",
    "# Model\n",
    "EMBEDDING_SIZE = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50000\n",
    "LR = 3e-4\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset\n",
    "# ----------------------------\n",
    "def generate_batch(batch_size):\n",
    "    a = torch.randint(0, VOCAB_SIZE - 1, (batch_size,))\n",
    "    b = torch.randint(1, VOCAB_SIZE - 1, (batch_size,))\n",
    "    y = a % b\n",
    "    x = torch.stack([a, b, torch.full_like(a, SEP_TOKEN)], dim=1)\n",
    "    return x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "# ----------------------------\n",
    "# Multi-Head Self Attention\n",
    "# ----------------------------\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_embedding, num_heads):\n",
    "        super().__init__()\n",
    "        assert num_embedding % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = num_embedding // num_heads\n",
    "        self.qkv = nn.Linear(num_embedding, 3 * num_embedding)\n",
    "        self.out = nn.Linear(num_embedding, num_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch size, sequence length, embedding size)\n",
    "        B, T, C = x.shape\n",
    "        # Calculate key, value, query tensors\n",
    "        qkv = self.qkv(x)   # (batch size, sequence length, 3 * embedding size)\n",
    "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim)   # (batch size, sequence length, 3, number heads, head dimensions)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)    # (3, batch size, number heads, sequence length, head dimensions)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]    # for each -> (batch size, number heads, sequence length, head dimensions)\n",
    "        # Scaled dot product attention\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)   # (batch size, number heads, sequence length, sequence length)\n",
    "        attn = torch.softmax(scores, dim=-1)   # (batch size, number heads, sequence length, sequence length)\n",
    "        # Output\n",
    "        out = attn @ v  # (batch size, number heads, sequence length, head dimensions)\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)  # (batch size, sequence length, embedding size)\n",
    "        out = self.out(out)  # (batch size, sequence length, embedding size)\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer Block\n",
    "# ----------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_embedding, num_heads, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(num_embedding)\n",
    "        self.attn = MultiHeadSelfAttention(num_embedding, num_heads)\n",
    "        self.ln_2 = nn.LayerNorm(num_embedding)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_embedding, mlp_ratio * num_embedding),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * num_embedding, num_embedding),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch size, sequence length, embedding size)\n",
    "        x = x + self.attn(self.ln_1(x)) # (batch size, sequence length, embedding size)\n",
    "        x = x + self.mlp(self.ln_2(x))  # (batch size, sequence length, embedding size)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Full Transformer Model\n",
    "# ----------------------------\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "        self.pos_emb = nn.Embedding(MAX_LEN, EMBEDDING_SIZE)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(EMBEDDING_SIZE, NUM_HEADS)\n",
    "            for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(EMBEDDING_SIZE)\n",
    "        self.head = nn.Linear(EMBEDDING_SIZE, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch size, sequence length)\n",
    "        _, T = x.shape\n",
    "        # Embeddings\n",
    "        positions = torch.arange(T, device=DEVICE)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions) # (batch size, sequence length, embedding size)\n",
    "        # Transformer\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)    # (batch size, sequence length, embedding size)\n",
    "        # Normalization\n",
    "        x = self.norm(x)    # (batch size, sequence length, embedding size)\n",
    "        # Use first token representation for classification\n",
    "        out = self.head(x[:, 0, :])\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "model = TransformerModel().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "x_train, y_train = generate_batch(BATCH_SIZE)\n",
    "x_val, y_val = generate_batch(BATCH_SIZE)\n",
    "losses_train_list, losses_val_list = [], []\n",
    "acc_train_list, acc_val_list = [], []\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # -------------------- Train --------------------\n",
    "    model.train()\n",
    "    logits_train = model(x_train)\n",
    "    loss_train = F.cross_entropy(logits_train, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    losses_train_list.append(loss_train.item())\n",
    "    # Training accuracy\n",
    "    preds_train = logits_train.argmax(dim=-1)\n",
    "    acc_train = (preds_train == y_train).float().mean().item()\n",
    "    acc_train_list.append(acc_train)\n",
    "\n",
    "    # -------------------- Eval --------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_val = model(x_val)\n",
    "        loss_val = F.cross_entropy(logits_val, y_val)\n",
    "        losses_val_list.append(loss_val.item())\n",
    "        # Evaluation accuracy\n",
    "        preds_val = logits_val.argmax(dim=-1)\n",
    "        acc_val = (preds_val == y_val).float().mean().item()\n",
    "        acc_val_list.append(acc_val)\n",
    "\n",
    "# ----------------------------\n",
    "# Manuel eval\n",
    "# ----------------------------\n",
    "# Generate one sample\n",
    "x_val, y_val = generate_batch(1)  # shape: (1, 3)\n",
    "logits_val = model(x_val)         # shape: (1, VOCAB_SIZE)\n",
    "# Convert logits to probabilities\n",
    "probs = F.softmax(logits_val, dim=-1)\n",
    "# Predicted class\n",
    "pred_class = probs.argmax(dim=-1)\n",
    "# Print\n",
    "print(\"Input sequence:\", x_val)\n",
    "print(\"True label:\", y_val)\n",
    "# print(\"Predicted logits:\", logits_val)\n",
    "# print(\"Predicted probabilities:\", probs)\n",
    "print(\"Predicted class:\", pred_class)\n",
    "\n",
    "# ----------------------------\n",
    "# Plotting\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Cross Entropy Losses\")\n",
    "plt.plot(losses_train_list, color=\"blue\")\n",
    "plt.plot(losses_val_list, color=\"orange\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(acc_train_list, color=\"blue\")\n",
    "plt.plot(acc_val_list, color=\"orange\")\n",
    "plt.grid()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
