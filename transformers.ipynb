{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ae7595",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be535bc5",
   "metadata": {},
   "source": [
    "### Example 1: learning modulo funcion / groking\n",
    "...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b72c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters\n",
    "# ----------------------------\n",
    "# Vocabulary\n",
    "VOCAB_SIZE = 10 + 1\n",
    "SEP_TOKEN = VOCAB_SIZE - 1\n",
    "MAX_LEN = 3                 # [a, b, SEP]\n",
    "# Model\n",
    "D_MODEL = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "D_FF = 128\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 300\n",
    "LR = 3e-4\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset\n",
    "# ----------------------------\n",
    "def generate_batch(batch_size):\n",
    "    a = torch.randint(0, VOCAB_SIZE - 1, (batch_size,))\n",
    "    b = torch.randint(1, VOCAB_SIZE - 1, (batch_size,))\n",
    "    y = a % b\n",
    "    x = torch.stack([a, b, torch.full_like(a, SEP_TOKEN)], dim=1)\n",
    "    return x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "# ----------------------------\n",
    "# Multi-Head Self Attention\n",
    "# ----------------------------\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = attn @ v\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out(out)\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer Block\n",
    "# ----------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Full Transformer Model\n",
    "# ----------------------------\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(VOCAB_SIZE, D_MODEL)\n",
    "        self.pos_emb = nn.Embedding(MAX_LEN, D_MODEL)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(D_MODEL, NUM_HEADS, D_FF)\n",
    "            for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(D_MODEL)\n",
    "        self.head = nn.Linear(D_MODEL, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "\n",
    "        positions = torch.arange(T, device=DEVICE)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Use first token representation for classification\n",
    "        return self.head(x[:, 0, :])\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "model = TransformerModel().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    x, y = generate_batch(BATCH_SIZE)\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, y = generate_batch(total)\n",
    "    preds = model(x).argmax(dim=-1)\n",
    "    correct = (preds == y).sum().item()\n",
    "\n",
    "print(f\"\\nAccuracy: {correct/total:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
